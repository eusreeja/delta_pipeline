services:
  spark-master:
    image: bde2020/spark-master:3.3.0-hadoop3.3
    container_name: spark-master
    ports:
      - "8080:8080"  # Spark Master Web UI
      - "7077:7077"  # Spark Master port
    environment:
      - INIT_DAEMON_STEP=setup_spark
    networks:
      - datapipeline-network
    
  spark-worker-1:
    image: bde2020/spark-worker:3.3.0-hadoop3.3
    container_name: spark-worker-1
    depends_on:
      - spark-master
    ports:
      - "8081:8081"  # Spark Worker Web UI
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"
    networks:
      - datapipeline-network

  datapipeline-app:
    build: 
      context: .
      dockerfile: Dockerfile
    image: datapipeline-energy:latest
    container_name: datapipeline-app
    depends_on:
      - spark-master
      - spark-worker-1
    ports:
      - "8888:8888"  # Jupyter Lab for exploration
      - "4040:4040"  # Spark Application UI
    environment:
      - SPARK_MASTER_URL=spark://spark-master:7077
      - JUPYTER_MODE=true
    volumes:
      - ./delta_lake:/app/delta_lake
      - ./logs:/app/logs
      - ./notebooks:/app/notebooks
    networks:
      - datapipeline-network
    command: >
      bash -c "
        echo 'Waiting for Spark cluster to be ready...' &&
        sleep 15 &&
        echo 'Starting Energy Data Pipeline Demo...' &&
        echo 'Spark Master UI: http://localhost:8080' &&
        echo 'Jupyter Lab: http://localhost:8888' &&
        /entrypoint.sh
      "

networks:
  datapipeline-network:
    driver: bridge
