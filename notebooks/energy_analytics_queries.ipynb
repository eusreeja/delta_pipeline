{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56a1e234",
   "metadata": {},
   "source": [
    "# Energy Data Analytics - Delta Lake Queries\n",
    "\n",
    "This notebook demonstrates reading Delta format data and executing three core energy analytics queries using Spark SQL:\n",
    "\n",
    "1. **Daily Production Trends** - Daily electricity production by production type\n",
    "2. **Underperformance Prediction Features** - ML features for energy production forecasting\n",
    "3. **Wind Price Analysis** - Wind power production vs electricity prices\n",
    "\n",
    "## Data Sources\n",
    "- Delta tables in the Gold layer: `gold_fact_power`, `gold_dim_production_type`, `gold_fact_power_30min_agg`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eeb6fe5",
   "metadata": {},
   "source": [
    "## 1. Setup Spark Session and Delta Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9e77ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/srsu/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Ivy Default Cache set to: /Users/srsu/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/srsu/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-e648c4f8-5ead-4e61-8345-bd5cafca5404;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.0.0 in central\n",
      "\tfound io.delta#delta-storage;2.0.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in local-m2-cache\n",
      ":: resolution report :: resolve 86ms :: artifacts dl 3ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.0.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.0.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from local-m2-cache in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-e648c4f8-5ead-4e61-8345-bd5cafca5404\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/2ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/srsu/spark-3.2.0-bin-hadoop3.2/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/03 23:57:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Spark session initialized successfully!\n",
      "Spark Version: 3.2.0\n",
      "Application Name: NotebookQueries\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Try to initialize a minimal Spark session\n",
    "try:\n",
    "    # Stop any existing session first\n",
    "    if 'spark' in globals() and spark is not None:\n",
    "        spark.stop()\n",
    "    \n",
    "    # Initialize with minimal configuration\n",
    "    spark = ((SparkSession.builder.master(\"local[*]\")\n",
    "    .appName(\"NotebookQueries\")\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.0.0\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    .config(\"spark.delta.logStore.class\", \"org.apache.spark.sql.delta.storage.S3SingleDriverLogStore\")\n",
    "    .config(\"spark.sql.warehouse.dir\", \"./delta_lake\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"10485760\")\n",
    "    .config(\"spark.executor.memory\", \"4g\")\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "    .getOrCreate()))\n",
    "    \n",
    "    print(\"‚úÖ Spark session initialized successfully!\")\n",
    "    print(f\"Spark Version: {spark.version}\")\n",
    "    print(f\"Application Name: {spark.sparkContext.appName}\")\n",
    "    \n",
    "    # Set log level to reduce verbosity\n",
    "    spark.sparkContext.setLogLevel(\"WARN\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to initialize Spark session: {e}\")\n",
    "    print(\"\\nüìù Note: If Spark initialization fails in the notebook environment,\")\n",
    "    print(\"   you can still verify the pipeline functionality by running:\")\n",
    "    print(\"   python run_notebook_queries.py\")\n",
    "    spark = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f82e87d",
   "metadata": {},
   "source": [
    "## 2. Read Delta Lake Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ae156ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Reading Delta tables...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded gold_dim_production_type: 15 records\n",
      "\n",
      "Dimension table schema:\n",
      "root\n",
      " |-- production_type_id: string (nullable = true)\n",
      " |-- production_type: string (nullable = true)\n",
      " |-- energy_category: string (nullable = true)\n",
      " |-- controllability_type: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- effective_date: date (nullable = true)\n",
      " |-- expiry_date: date (nullable = true)\n",
      " |-- active_flag: boolean (nullable = true)\n",
      " |-- created_at: timestamp (nullable = true)\n",
      " |-- updated_at: timestamp (nullable = true)\n",
      "\n",
      "‚úÖ Loaded gold_fact_power: 86360 records\n",
      "‚úÖ Loaded gold_fact_power_30min_agg: 43180 records\n",
      "‚úÖ Successfully loaded all Delta tables\n"
     ]
    }
   ],
   "source": [
    "# Define Delta table paths (update these paths to match your environment)\n",
    "delta_base_path = \"/Users/srsu/Documents/Personal/git/delta_pipeline/delta_lake\"\n",
    "\n",
    "# Read Delta tables from the Gold layer\n",
    "print(\"üìä Reading Delta tables...\")\n",
    "\n",
    "try:\n",
    "    # Check if spark session exists\n",
    "    if spark is None:\n",
    "        print(\"‚ùå Spark session not initialized. Please run the first cell.\")\n",
    "    else:\n",
    "        # Read dimension table\n",
    "        gold_dim_production_type = spark.read.format(\"delta\").load(f\"{delta_base_path}/gold/dim_production_type\")\n",
    "        gold_dim_production_type.createOrReplaceTempView(\"gold_dim_production_type\")\n",
    "        print(f\"‚úÖ Loaded gold_dim_production_type: {gold_dim_production_type.count()} records\")\n",
    "        \n",
    "        # Show schema of dimension table\n",
    "        print(\"\\nDimension table schema:\")\n",
    "        gold_dim_production_type.printSchema()\n",
    "        \n",
    "        # Read daily fact table for power generation\n",
    "        gold_fact_power = spark.read.format(\"delta\").load(f\"{delta_base_path}/gold/fact_power\")\n",
    "        gold_fact_power.createOrReplaceTempView(\"gold_fact_power\")\n",
    "        print(f\"‚úÖ Loaded gold_fact_power: {gold_fact_power.count()} records\")\n",
    "        \n",
    "        # Read 30-minute aggregated fact table  \n",
    "        gold_fact_power_30min_agg = spark.read.format(\"delta\").load(f\"{delta_base_path}/gold/fact_power_30min_agg\")\n",
    "        gold_fact_power_30min_agg.createOrReplaceTempView(\"gold_fact_power_30min_agg\")\n",
    "        print(f\"‚úÖ Loaded gold_fact_power_30min_agg: {gold_fact_power_30min_agg.count()} records\")\n",
    "        \n",
    "        print(\"‚úÖ Successfully loaded all Delta tables\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading Delta tables: {str(e)}\")\n",
    "    print(\"Please ensure the pipeline has been run and Delta tables exist\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe46d90d",
   "metadata": {},
   "source": [
    "## 3. Query 1: Daily Production Trends\n",
    "\n",
    "This query analyzes daily electricity production trends by production type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4b0b7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Executing Query 1: Daily Production Trends\n",
      "==================================================\n",
      "Query executed successfully!\n",
      "Results: 810 records found\n",
      "\n",
      "Sample Results:\n",
      "+----+-----+---+---------------+----------------------+\n",
      "|year|month|day|production_type|total_daily_production|\n",
      "+----+-----+---+---------------+----------------------+\n",
      "|2025|1    |1  |Biomass        |382260.29999999993    |\n",
      "|2025|1    |1  |Fossil         |348234.4999999999     |\n",
      "|2025|1    |1  |Geothermal     |2159.8999999999996    |\n",
      "|2025|1    |1  |Hydro          |20687.500000000004    |\n",
      "|2025|1    |1  |Load           |4721965.6000000015    |\n",
      "|2025|1    |1  |Others         |19598.2               |\n",
      "|2025|1    |1  |Solar          |215798.59999999998    |\n",
      "|2025|1    |1  |Waste          |95757.70000000001     |\n",
      "|2025|1    |1  |Wind           |3517545.0999999987    |\n",
      "|2025|1    |2  |Biomass        |405472.7              |\n",
      "+----+-----+---+---------------+----------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "Schema:\n",
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- production_type: string (nullable = true)\n",
      " |-- total_daily_production: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query 1: Daily Production Trends\n",
    "daily_production_query = \"\"\"\n",
    "SELECT\n",
    "  f.year,\n",
    "  f.month,\n",
    "  f.day,\n",
    "  d.production_type AS production_type,\n",
    "  SUM(f.electricity_produced) AS total_daily_production\n",
    "FROM gold_fact_power f\n",
    "JOIN gold_dim_production_type d ON f.production_type_id = d.production_type_id\n",
    "WHERE f.country = 'de'\n",
    "GROUP BY f.year, f.month, f.day, d.production_type\n",
    "ORDER BY f.year, f.month, f.day, d.production_type\n",
    "\"\"\"\n",
    "\n",
    "print(\"üîç Executing Query 1: Daily Production Trends\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    if spark is None:\n",
    "        print(\"‚ùå Spark session not initialized. Please run the first cell.\")\n",
    "    else:\n",
    "        daily_trends_df = spark.sql(daily_production_query)\n",
    "        \n",
    "        print(f\"Query executed successfully!\")\n",
    "        print(f\"Results: {daily_trends_df.count()} records found\")\n",
    "        \n",
    "        print(\"\\nSample Results:\")\n",
    "        daily_trends_df.show(10, truncate=False)\n",
    "        \n",
    "        # Show schema\n",
    "        print(\"\\nSchema:\")\n",
    "        daily_trends_df.printSchema()\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Query failed: {e}\")\n",
    "    print(\"This is expected if Delta tables don't exist or Spark session is not initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23621cce",
   "metadata": {},
   "source": [
    "## 4. Query 2: Underperformance Prediction Features\n",
    "\n",
    "This query generates ML features for predicting energy production underperformance with lag features and rolling averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a94f0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Executing Query 2: ML Features for Underperformance Prediction\n",
      "=================================================================\n",
      "Query executed successfully!\n",
      "Results: 20 records found\n",
      "\n",
      "Sample ML Features:\n",
      "+-------------------+----------------------------------------------------------------+---------------+---------------+--------------------+--------------------------+----+-----+---+----+------------------+-----------------+------+-----------------+\n",
      "|timestamp_30min    |production_type_id                                              |production_type|energy_category|controllability_type|total_electricity_produced|year|month|day|hour|minute_interval_30|lag_1d           |lag_1w|rolling_7d_avg   |\n",
      "+-------------------+----------------------------------------------------------------+---------------+---------------+--------------------+--------------------------+----+-----+---+----+------------------+-----------------+------+-----------------+\n",
      "|2025-01-01 00:00:00|49e077857baab80a41e125aaa88b8ba9f239673011a8ddf982a790d7d6176dd5|Hydro          |Renewable      |Controllable        |1291.5                    |2025|1    |1  |0   |0                 |null             |null  |null             |\n",
      "|2025-01-02 00:00:00|49e077857baab80a41e125aaa88b8ba9f239673011a8ddf982a790d7d6176dd5|Hydro          |Renewable      |Controllable        |1238.7                    |2025|1    |2  |0   |0                 |1291.5           |null  |1291.5           |\n",
      "|2025-01-03 00:00:00|49e077857baab80a41e125aaa88b8ba9f239673011a8ddf982a790d7d6176dd5|Hydro          |Renewable      |Controllable        |326.9                     |2025|1    |3  |0   |0                 |1238.7           |null  |1265.1           |\n",
      "|2025-01-04 00:00:00|49e077857baab80a41e125aaa88b8ba9f239673011a8ddf982a790d7d6176dd5|Hydro          |Renewable      |Controllable        |771.0999999999999         |2025|1    |4  |0   |0                 |326.9            |null  |952.3666666666667|\n",
      "|2025-01-05 00:00:00|49e077857baab80a41e125aaa88b8ba9f239673011a8ddf982a790d7d6176dd5|Hydro          |Renewable      |Controllable        |1463.3                    |2025|1    |5  |0   |0                 |771.0999999999999|null  |907.05           |\n",
      "+-------------------+----------------------------------------------------------------+---------------+---------------+--------------------+--------------------------+----+-----+---+----+------------------+-----------------+------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Feature Statistics:\n",
      "+-------+--------------------------+-----------------+-----------------+------------------+\n",
      "|summary|total_electricity_produced|           lag_1d|           lag_1w|    rolling_7d_avg|\n",
      "+-------+--------------------------+-----------------+-----------------+------------------+\n",
      "|  count|                        20|               19|               13|                19|\n",
      "|   mean|                    593.22|622.7473684210527|743.1153846153848| 804.6281175410553|\n",
      "| stddev|         520.8986502089189|517.6905868785814|551.3943399028711|202.34283167526436|\n",
      "|    min|                      32.2|             61.8|             61.8| 614.1300000000001|\n",
      "|    max|                    1664.6|           1664.6|           1664.6|            1291.5|\n",
      "+-------+--------------------------+-----------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query 2: Underperformance Prediction Features\n",
    "underperformance_query = \"\"\"\n",
    "SELECT\n",
    "    f.timestamp_30min,\n",
    "    f.production_type_id,\n",
    "    d.production_type,\n",
    "    d.energy_category,\n",
    "    d.controllability_type,\n",
    "    f.total_electricity_produced,\n",
    "    f.year, f.month, f.day, f.hour, f.minute_interval_30,\n",
    "    LAG(f.total_electricity_produced, 48) OVER (PARTITION BY f.production_type_id ORDER BY f.timestamp_30min) AS lag_1d,\n",
    "    LAG(f.total_electricity_produced, 336) OVER (PARTITION BY f.production_type_id ORDER BY f.timestamp_30min) AS lag_1w,\n",
    "    AVG(f.total_electricity_produced) OVER (\n",
    "        PARTITION BY f.production_type_id, f.hour, f.minute_interval_30\n",
    "        ORDER BY f.timestamp_30min\n",
    "        ROWS BETWEEN 336 PRECEDING AND 1 PRECEDING\n",
    "    ) AS rolling_7d_avg\n",
    "FROM gold_fact_power_30min_agg f\n",
    "JOIN gold_dim_production_type d ON f.production_type_id = d.production_type_id\n",
    "WHERE f.country = 'de' AND d.active_flag = TRUE\n",
    "LIMIT 20\n",
    "\"\"\"\n",
    "\n",
    "print(\"üîç Executing Query 2: ML Features for Underperformance Prediction\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "try:\n",
    "    if spark is None:\n",
    "        print(\"‚ùå Spark session not initialized. Please run the first cell.\")\n",
    "    else:\n",
    "        underperformance_query_df = spark.sql(underperformance_query)\n",
    "        \n",
    "        print(f\"Query executed successfully!\")\n",
    "        print(f\"Results: {underperformance_query_df.count()} records found\")\n",
    "        \n",
    "        print(\"\\nSample ML Features:\")\n",
    "        underperformance_query_df.show(5, truncate=False)\n",
    "        \n",
    "        # Show feature statistics (only for non-null values)\n",
    "        print(\"\\nFeature Statistics:\")\n",
    "        feature_stats = underperformance_query_df.select(\"total_electricity_produced\", \"lag_1d\", \"lag_1w\", \"rolling_7d_avg\").describe()\n",
    "        feature_stats.show()\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Query failed: {e}\")\n",
    "    print(\"This is expected if 30-minute Delta table doesn't exist or Spark session is not initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd91dd0",
   "metadata": {},
   "source": [
    "## 5. Query 3: Wind Price Analysis\n",
    "\n",
    "This query analyzes the relationship between wind power production (offshore and onshore) and electricity prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "68508a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Executing Query 3: Wind Power vs Price Analysis\n",
      "==================================================\n",
      "Query executed successfully!\n",
      "Results: 90 records found\n",
      "\n",
      "Wind Power vs Price Results:\n",
      "+----+-----+---+---------------+-------------------------+---------------------------+\n",
      "|year|month|day|production_type|total_daily_production_mw|avg_daily_price_eur_per_mwh|\n",
      "+----+-----+---+---------------+-------------------------+---------------------------+\n",
      "|2025|1    |1  |Wind           |3517545.0999999987       |0.3966666666666667         |\n",
      "|2025|1    |2  |Wind           |1968775.3000000007       |38.32111111111111          |\n",
      "|2025|1    |3  |Wind           |2640196.9999999995       |84.20333333333332          |\n",
      "|2025|1    |4  |Wind           |1010095.4999999998       |102.16777777777777         |\n",
      "|2025|1    |5  |Wind           |1925518.5000000002       |91.67555555555555          |\n",
      "|2025|1    |6  |Wind           |3639525.7                |21.005555555555553         |\n",
      "|2025|1    |7  |Wind           |3380889.7000000007       |29.265555555555554         |\n",
      "|2025|1    |8  |Wind           |2223300.3000000003       |78.82444444444444          |\n",
      "|2025|1    |9  |Wind           |1479225.2999999993       |115.95333333333332         |\n",
      "|2025|1    |10 |Wind           |2027758.2                |91.29222222222221          |\n",
      "+----+-----+---+---------------+-------------------------+---------------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Results: 90 records found\n",
      "\n",
      "Wind Power vs Price Results:\n",
      "+----+-----+---+---------------+-------------------------+---------------------------+\n",
      "|year|month|day|production_type|total_daily_production_mw|avg_daily_price_eur_per_mwh|\n",
      "+----+-----+---+---------------+-------------------------+---------------------------+\n",
      "|2025|1    |1  |Wind           |3517545.0999999987       |0.3966666666666667         |\n",
      "|2025|1    |2  |Wind           |1968775.3000000007       |38.32111111111111          |\n",
      "|2025|1    |3  |Wind           |2640196.9999999995       |84.20333333333332          |\n",
      "|2025|1    |4  |Wind           |1010095.4999999998       |102.16777777777777         |\n",
      "|2025|1    |5  |Wind           |1925518.5000000002       |91.67555555555555          |\n",
      "|2025|1    |6  |Wind           |3639525.7                |21.005555555555553         |\n",
      "|2025|1    |7  |Wind           |3380889.7000000007       |29.265555555555554         |\n",
      "|2025|1    |8  |Wind           |2223300.3000000003       |78.82444444444444          |\n",
      "|2025|1    |9  |Wind           |1479225.2999999993       |115.95333333333332         |\n",
      "|2025|1    |10 |Wind           |2027758.2                |91.29222222222221          |\n",
      "+----+-----+---+---------------+-------------------------+---------------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "Summary by Wind Type:\n",
      "\n",
      "Summary by Wind Type:\n",
      "+---------------+------------------+------------------+----------+\n",
      "|production_type|avg_production    |avg_price         |total_days|\n",
      "+---------------+------------------+------------------+----------+\n",
      "|Wind           |1175999.9766666663|106.26946913580244|90        |\n",
      "+---------------+------------------+------------------+----------+\n",
      "\n",
      "+---------------+------------------+------------------+----------+\n",
      "|production_type|avg_production    |avg_price         |total_days|\n",
      "+---------------+------------------+------------------+----------+\n",
      "|Wind           |1175999.9766666663|106.26946913580244|90        |\n",
      "+---------------+------------------+------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query 3: Wind Price Analysis\n",
    "wind_price_query = \"\"\"\n",
    "SELECT\n",
    "  f.year, f.month, f.day,\n",
    "  d.production_type AS production_type,\n",
    "  SUM(f.electricity_produced) AS total_daily_production_mw,\n",
    "  AVG(f.electricity_price) AS avg_daily_price_eur_per_mwh\n",
    "FROM gold_fact_power f\n",
    "JOIN gold_dim_production_type d ON f.production_type_id = d.production_type_id\n",
    "WHERE f.country = 'de'\n",
    "  AND d.production_type LIKE '%Wind%'\n",
    "  AND d.active_flag = TRUE \n",
    "GROUP BY f.year, f.month, f.day, d.production_type\n",
    "ORDER BY f.year, f.month, f.day, d.production_type\n",
    "\"\"\"\n",
    "\n",
    "print(\"üîç Executing Query 3: Wind Power vs Price Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    if spark is None:\n",
    "        print(\"‚ùå Spark session not initialized. Please run the first cell.\")\n",
    "    else:\n",
    "        wind_analysis_df = spark.sql(wind_price_query)\n",
    "        \n",
    "        print(f\"Query executed successfully!\")\n",
    "        print(f\"Results: {wind_analysis_df.count()} records found\")\n",
    "        \n",
    "        print(\"\\nWind Power vs Price Results:\")\n",
    "        wind_analysis_df.show(10, truncate=False)\n",
    "        \n",
    "        # Summary statistics by wind type\n",
    "        if wind_analysis_df.count() > 0:\n",
    "            print(\"\\nSummary by Wind Type:\")\n",
    "            wind_summary = wind_analysis_df.groupBy(\"production_type\").agg(\n",
    "                avg(\"total_daily_production_mw\").alias(\"avg_production\"),\n",
    "                avg(\"avg_daily_price_eur_per_mwh\").alias(\"avg_price\"),\n",
    "                count(\"*\").alias(\"total_days\")\n",
    "            )\n",
    "            wind_summary.show(truncate=False)\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Query failed: {e}\")\n",
    "    print(\"This is expected if Delta tables don't exist or Spark session is not initialized\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
