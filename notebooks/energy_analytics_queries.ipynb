{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56a1e234",
   "metadata": {},
   "source": [
    "# Energy Data Analytics - Delta Lake Queries\n",
    "\n",
    "This notebook demonstrates reading Delta format data and executing three core energy analytics queries using Spark SQL:\n",
    "\n",
    "1. **Daily Production Trends** - Daily electricity production by production type\n",
    "2. **Underperformance Prediction Features** - ML features for energy production forecasting\n",
    "3. **Wind Price Analysis** - Wind power production vs electricity prices\n",
    "\n",
    "## Data Sources\n",
    "- Delta tables in the Gold layer: `gold_fact_power`, `gold_dim_production_type`, `gold_fact_power_30min_agg`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eeb6fe5",
   "metadata": {},
   "source": [
    "## 1. Setup Spark Session and Delta Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9e77ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/sreejadath/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/sreejadath/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-4ef33f5c-d276-4c8a-ba57-360b444b806d;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.0.0 in central\n",
      "\tfound io.delta#delta-storage;2.0.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      ":: resolution report :: resolve 121ms :: artifacts dl 5ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.0.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.0.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-4ef33f5c-d276-4c8a-ba57-360b444b806d\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/5ms)\n",
      "25/07/01 14:40:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session created with Delta Lake support\n",
      "Spark Version: 3.5.4\n",
      "Application Name: pyspark-shell\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Create Spark session with Delta Lake configuration\n",
    "spark = (SparkSession.builder.master(\"local[*]\")\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.0.0\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    .config(\"spark.delta.logStore.class\", \"org.apache.spark.sql.delta.storage.S3SingleDriverLogStore\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")  # Increase to 4GB\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\")  # Reduce shuffle partitions\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"10485760\")  # 10MB\n",
    "    .config(\"spark.executor.memory\", \"4g\")  # If using executors\n",
    "    .getOrCreate())\n",
    "\n",
    "print(\"Spark Session created with Delta Lake support\")\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Application Name: {spark.sparkContext.appName}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f82e87d",
   "metadata": {},
   "source": [
    "## 2. Read Delta Lake Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ae156ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Reading Delta tables...\n",
      "Could not read Delta tables: An error occurred while calling o67.load.\n",
      ": com.google.common.util.concurrent.ExecutionError: java.lang.NoSuchMethodError: 'java.lang.String org.apache.spark.ErrorInfo.messageFormat()'\n",
      "\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2261)\n",
      "\tat com.google.common.cache.LocalCache.get(LocalCache.java:4000)\n",
      "\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4789)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog$.getDeltaLogFromCache$1(DeltaLog.scala:606)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:613)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:511)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:85)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:85)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.$anonfun$snapshot$3(DeltaTableV2.scala:114)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.snapshot$lzycompute(DeltaTableV2.scala:114)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.snapshot(DeltaTableV2.scala:102)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.toBaseRelation(DeltaTableV2.scala:168)\n",
      "\tat org.apache.spark.sql.delta.sources.DeltaDataSource.$anonfun$createRelation$5(DeltaDataSource.scala:211)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:141)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:139)\n",
      "\tat org.apache.spark.sql.delta.sources.DeltaDataSource.recordFrameProfile(DeltaDataSource.scala:51)\n",
      "\tat org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:169)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:186)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.NoSuchMethodError: 'java.lang.String org.apache.spark.ErrorInfo.messageFormat()'\n",
      "\tat org.apache.spark.sql.delta.DeltaThrowableHelper$.getMessage(DeltaThrowableHelper.scala:80)\n",
      "\tat org.apache.spark.sql.delta.DeltaFileNotFoundException.<init>(DeltaErrors.scala:2448)\n",
      "\tat org.apache.spark.sql.delta.DeltaErrorsBase.fileOrDirectoryNotFoundException(DeltaErrors.scala:446)\n",
      "\tat org.apache.spark.sql.delta.DeltaErrorsBase.fileOrDirectoryNotFoundException$(DeltaErrors.scala:443)\n",
      "\tat org.apache.spark.sql.delta.DeltaErrors$.fileOrDirectoryNotFoundException(DeltaErrors.scala:2264)\n",
      "\tat org.apache.spark.sql.delta.storage.S3SingleDriverLogStore.listFromInternal(S3SingleDriverLogStore.scala:122)\n",
      "\tat org.apache.spark.sql.delta.storage.S3SingleDriverLogStore.listFrom(S3SingleDriverLogStore.scala:141)\n",
      "\tat org.apache.spark.sql.delta.SnapshotManagement.listFrom(SnapshotManagement.scala:69)\n",
      "\tat org.apache.spark.sql.delta.SnapshotManagement.listFrom$(SnapshotManagement.scala:68)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.listFrom(DeltaLog.scala:65)\n",
      "\tat org.apache.spark.sql.delta.SnapshotManagement.listFromOrNone(SnapshotManagement.scala:86)\n",
      "\tat org.apache.spark.sql.delta.SnapshotManagement.listFromOrNone$(SnapshotManagement.scala:82)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.listFromOrNone(DeltaLog.scala:65)\n",
      "\tat org.apache.spark.sql.delta.SnapshotManagement.$anonfun$listDeltaAndCheckpointFiles$1(SnapshotManagement.scala:106)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:141)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:139)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:65)\n",
      "\tat org.apache.spark.sql.delta.SnapshotManagement.listDeltaAndCheckpointFiles(SnapshotManagement.scala:106)\n",
      "\tat org.apache.spark.sql.delta.SnapshotManagement.getLogSegmentForVersion(SnapshotManagement.scala:138)\n",
      "\tat org.apache.spark.sql.delta.SnapshotManagement.getLogSegmentForVersion$(SnapshotManagement.scala:133)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.getLogSegmentForVersion(DeltaLog.scala:65)\n",
      "\tat org.apache.spark.sql.delta.SnapshotManagement.getLogSegmentFrom(SnapshotManagement.scala:64)\n",
      "\tat org.apache.spark.sql.delta.SnapshotManagement.getLogSegmentFrom$(SnapshotManagement.scala:62)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.getLogSegmentFrom(DeltaLog.scala:65)\n",
      "\tat org.apache.spark.sql.delta.SnapshotManagement.$anonfun$getSnapshotAtInit$1(SnapshotManagement.scala:262)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:141)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:139)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:65)\n",
      "\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit(SnapshotManagement.scala:260)\n",
      "\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit$(SnapshotManagement.scala:258)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.getSnapshotAtInit(DeltaLog.scala:65)\n",
      "\tat org.apache.spark.sql.delta.SnapshotManagement.$init$(SnapshotManagement.scala:54)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.<init>(DeltaLog.scala:70)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$3(DeltaLog.scala:595)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$2(DeltaLog.scala:591)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:141)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:139)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog$.recordFrameProfile(DeltaLog.scala:460)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:134)\n",
      "\tat com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:77)\n",
      "\tat com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:67)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog$.recordOperation(DeltaLog.scala:460)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:133)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:123)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:111)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog$.recordDeltaOperation(DeltaLog.scala:460)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog$.createDeltaLog$1(DeltaLog.scala:590)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$4(DeltaLog.scala:606)\n",
      "\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4792)\n",
      "\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)\n",
      "\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)\n",
      "\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)\n",
      "\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2257)\n",
      "\t... 35 more\n",
      "\n",
      "Creating sample data for POC demonstration...\n"
     ]
    }
   ],
   "source": [
    "# Define Delta table paths (update these paths to match your environment)\n",
    "delta_base_path = \"/Users/srsu/Downloads/spark_datapipeline/delta_lake\"\n",
    "\n",
    "# Read Delta tables from the Gold layer\n",
    "print(\"📊 Reading Delta tables...\")\n",
    "\n",
    "try:\n",
    "    # Read dimension table\n",
    "    gold_dim_production_type = spark.read.format(\"delta\").load(f\"{delta_base_path}/gold/gold_dim_production_type\")\n",
    "    gold_dim_production_type.createOrReplaceTempView(\"gold_dim_production_type\")\n",
    "    print(f\"Loaded gold_dim_production_type: {gold_dim_production_type.count()} records\")\n",
    "    \n",
    "    # Read daily fact table\n",
    "    gold_fact_power = spark.read.format(\"delta\").load(f\"{delta_base_path}/gold/gold_fact_power\")\n",
    "    gold_fact_power.createOrReplaceTempView(\"gold_fact_power\")\n",
    "    print(f\"Loaded gold_fact_power: {gold_fact_power.count()} records\")\n",
    "    \n",
    "    # Read 30-minute aggregated fact table\n",
    "    gold_fact_power_30min_agg = spark.read.format(\"delta\").load(f\"{delta_base_path}/gold/gold_fact_power_30min_agg\")\n",
    "    gold_fact_power_30min_agg.createOrReplaceTempView(\"gold_fact_power_30min_agg\")\n",
    "    print(f\"Loaded gold_fact_power_30min_agg: {gold_fact_power_30min_agg.count()} records\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Could not read Delta tables: {e}\")\n",
    "    print(\"Creating sample data for POC demonstration...\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe46d90d",
   "metadata": {},
   "source": [
    "## 3. Query 1: Daily Production Trends\n",
    "\n",
    "This query analyzes daily electricity production trends by production type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4b0b7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Executing Query 1: Daily Production Trends\n",
      "==================================================\n",
      "Query failed: An error occurred while calling o44.sql.\n",
      ": java.lang.NoSuchMethodError: 'java.lang.Object org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(org.antlr.v4.runtime.ParserRuleContext, scala.Function0)'\n",
      "\tat io.delta.sql.parser.DeltaSqlAstBuilder.visitSingleStatement(DeltaSqlParser.scala:244)\n",
      "\tat io.delta.sql.parser.DeltaSqlAstBuilder.visitSingleStatement(DeltaSqlParser.scala:146)\n",
      "\tat io.delta.sql.parser.DeltaSqlBaseParser$SingleStatementContext.accept(DeltaSqlBaseParser.java:165)\n",
      "\tat org.antlr.v4.runtime.tree.AbstractParseTreeVisitor.visit(AbstractParseTreeVisitor.java:18)\n",
      "\tat io.delta.sql.parser.DeltaSqlParser.$anonfun$parsePlan$1(DeltaSqlParser.scala:74)\n",
      "\tat io.delta.sql.parser.DeltaSqlParser.parse(DeltaSqlParser.scala:103)\n",
      "\tat io.delta.sql.parser.DeltaSqlParser.parsePlan(DeltaSqlParser.scala:73)\n",
      "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$5(SparkSession.scala:684)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n",
      "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:683)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n",
      "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n",
      "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "This is expected if Delta tables don't exist - using sample data for POC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ANTLR Tool version 4.8 used for code generation does not match the current runtime version 4.9.3\n",
      "ANTLR Runtime version 4.8 used for parser compilation does not match the current runtime version 4.9.3\n",
      "ANTLR Tool version 4.8 used for code generation does not match the current runtime version 4.9.3\n",
      "ANTLR Runtime version 4.8 used for parser compilation does not match the current runtime version 4.9.3\n"
     ]
    }
   ],
   "source": [
    "# Query 1: Daily Production Trends\n",
    "daily_production_query = \"\"\"\n",
    "SELECT\n",
    "  f.year,\n",
    "  f.month,\n",
    "  f.day,\n",
    "  d.production_plant_name AS production_type,\n",
    "  SUM(f.electricity_produced) AS total_daily_production\n",
    "FROM gold_fact_power f\n",
    "JOIN gold_dim_production_type d ON f.production_type_id = d.production_type_id\n",
    "WHERE f.country = 'de'\n",
    "GROUP BY f.year, f.month, f.day, d.production_plant_name\n",
    "ORDER BY f.year, f.month, f.day, d.production_plant_name\n",
    "\"\"\"\n",
    "\n",
    "print(\"🔍 Executing Query 1: Daily Production Trends\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    daily_trends_df = spark.sql(daily_production_query)\n",
    "    \n",
    "    print(f\"Query executed successfully!\")\n",
    "    print(f\"Results: {daily_trends_df.count()} records found\")\n",
    "    \n",
    "    print(\"\\nSample Results:\")\n",
    "    daily_trends_df.show(10, truncate=False)\n",
    "    \n",
    "    # Show schema\n",
    "    print(\"\\nSchema:\")\n",
    "    daily_trends_df.printSchema()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Query failed: {e}\")\n",
    "    print(\"This is expected if Delta tables don't exist - using sample data for POC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23621cce",
   "metadata": {},
   "source": [
    "## 4. Query 2: Underperformance Prediction Features\n",
    "\n",
    "This query generates ML features for predicting energy production underperformance with lag features and rolling averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a94f0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Executing Query 2: ML Features for Underperformance Prediction\n",
      "=================================================================\n",
      " Query failed: An error occurred while calling o44.sql.\n",
      ": java.lang.NoSuchMethodError: 'java.lang.Object org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(org.antlr.v4.runtime.ParserRuleContext, scala.Function0)'\n",
      "\tat io.delta.sql.parser.DeltaSqlAstBuilder.visitSingleStatement(DeltaSqlParser.scala:244)\n",
      "\tat io.delta.sql.parser.DeltaSqlAstBuilder.visitSingleStatement(DeltaSqlParser.scala:146)\n",
      "\tat io.delta.sql.parser.DeltaSqlBaseParser$SingleStatementContext.accept(DeltaSqlBaseParser.java:165)\n",
      "\tat org.antlr.v4.runtime.tree.AbstractParseTreeVisitor.visit(AbstractParseTreeVisitor.java:18)\n",
      "\tat io.delta.sql.parser.DeltaSqlParser.$anonfun$parsePlan$1(DeltaSqlParser.scala:74)\n",
      "\tat io.delta.sql.parser.DeltaSqlParser.parse(DeltaSqlParser.scala:103)\n",
      "\tat io.delta.sql.parser.DeltaSqlParser.parsePlan(DeltaSqlParser.scala:73)\n",
      "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$5(SparkSession.scala:684)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n",
      "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:683)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n",
      "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n",
      "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "This is expected if 30-minute Delta table doesn't exist - using sample data for POC\n"
     ]
    }
   ],
   "source": [
    "# Query 2: Underperformance Prediction Features\n",
    "underperformance_query = \"\"\"\n",
    "SELECT\n",
    "    f.timestamp_30min,\n",
    "    f.production_type_id,\n",
    "    d.production_plant_name,\n",
    "    d.energy_category,\n",
    "    d.controllability_type,\n",
    "    f.total_electricity_produced,\n",
    "    f.year, f.month, f.day, f.hour, f.minute_interval_30,\n",
    "    LAG(f.total_electricity_produced, 48) OVER (PARTITION BY f.production_type_id ORDER BY f.timestamp_30min) AS lag_1d,\n",
    "    LAG(f.total_electricity_produced, 336) OVER (PARTITION BY f.production_type_id ORDER BY f.timestamp_30min) AS lag_1w,\n",
    "    AVG(f.total_electricity_produced) OVER (\n",
    "        PARTITION BY f.production_type_id, f.hour, f.minute_interval_30\n",
    "        ORDER BY f.timestamp_30min\n",
    "        RANGE BETWEEN 336 PRECEDING AND 1 PRECEDING\n",
    "    ) AS rolling_7d_avg\n",
    "FROM gold_fact_power_30min_agg f\n",
    "JOIN gold_dim_production_type d ON f.production_type_id = d.production_type_id\n",
    "WHERE f.country = 'de' AND d.active_flag = TRUE\n",
    "\"\"\"\n",
    "\n",
    "print(\"🔍 Executing Query 2: ML Features for Underperformance Prediction\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "try:\n",
    "    underperformance_query_df = spark.sql(underperformance_query)\n",
    "    \n",
    "    print(f\"Query executed successfully!\")\n",
    "    print(f\"Results: {underperformance_query_df.count()} records found\")\n",
    "    \n",
    "    print(\"\\nSample ML Features:\")\n",
    "    underperformance_query_df.show(5, truncate=False)\n",
    "    \n",
    "    # Show feature statistics\n",
    "    print(\"\\nFeature Statistics:\")\n",
    "    underperformance_query_df.select(\"total_electricity_produced\", \"lag_1d\", \"lag_1w\", \"rolling_7d_avg\").describe().show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\" Query failed: {e}\")\n",
    "    print(\"This is expected if 30-minute Delta table doesn't exist - using sample data for POC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd91dd0",
   "metadata": {},
   "source": [
    "## 5. Query 3: Wind Price Analysis\n",
    "\n",
    "This query analyzes the relationship between wind power production (offshore and onshore) and electricity prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68508a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Executing Query 3: Wind Power vs Price Analysis\n",
      "==================================================\n",
      "Query failed: An error occurred while calling o44.sql.\n",
      ": java.lang.NoSuchMethodError: 'java.lang.Object org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(org.antlr.v4.runtime.ParserRuleContext, scala.Function0)'\n",
      "\tat io.delta.sql.parser.DeltaSqlAstBuilder.visitSingleStatement(DeltaSqlParser.scala:244)\n",
      "\tat io.delta.sql.parser.DeltaSqlAstBuilder.visitSingleStatement(DeltaSqlParser.scala:146)\n",
      "\tat io.delta.sql.parser.DeltaSqlBaseParser$SingleStatementContext.accept(DeltaSqlBaseParser.java:165)\n",
      "\tat org.antlr.v4.runtime.tree.AbstractParseTreeVisitor.visit(AbstractParseTreeVisitor.java:18)\n",
      "\tat io.delta.sql.parser.DeltaSqlParser.$anonfun$parsePlan$1(DeltaSqlParser.scala:74)\n",
      "\tat io.delta.sql.parser.DeltaSqlParser.parse(DeltaSqlParser.scala:103)\n",
      "\tat io.delta.sql.parser.DeltaSqlParser.parsePlan(DeltaSqlParser.scala:73)\n",
      "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$5(SparkSession.scala:684)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n",
      "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:683)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n",
      "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n",
      "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "This is expected if Delta tables don't exist - using sample data for POC\n"
     ]
    }
   ],
   "source": [
    "# Query 3: Wind Price Analysis\n",
    "wind_price_query = \"\"\"\n",
    "SELECT\n",
    "  f.year, f.month, f.day,\n",
    "  d.production_plant_name AS production_type,\n",
    "  SUM(f.electricity_produced) AS total_daily_production_mw,\n",
    "  AVG(f.electricity_price) AS avg_daily_price_eur_per_mwh\n",
    "FROM gold_fact_power f\n",
    "JOIN gold_dim_production_type d ON f.production_type_id = d.production_type_id\n",
    "WHERE f.country = 'de'\n",
    "  AND d.production_plant_name IN ('Wind_Offshore', 'Wind_Onshore') \n",
    "  AND d.active_flag = TRUE \n",
    "GROUP BY f.year, f.month, f.day, d.production_plant_name\n",
    "ORDER BY f.year, f.month, f.day, d.production_plant_name\n",
    "\"\"\"\n",
    "\n",
    "print(\"🔍 Executing Query 3: Wind Power vs Price Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    wind_analysis_df = spark.sql(wind_price_query)\n",
    "    \n",
    "    print(f\"Query executed successfully!\")\n",
    "    print(f\"Results: {wind_analysis_df.count()} records found\")\n",
    "    \n",
    "    print(\"\\nWind Power vs Price Results:\")\n",
    "    wind_analysis_df.show(10, truncate=False)\n",
    "    \n",
    "    # Summary statistics by wind type\n",
    "    if wind_analysis_df.count() > 0:\n",
    "        print(\"\\nSummary by Wind Type:\")\n",
    "        wind_summary = wind_analysis_df.groupBy(\"production_type\").agg(\n",
    "            avg(\"total_daily_production_mw\").alias(\"avg_production\"),\n",
    "            avg(\"avg_daily_price_eur_per_mwh\").alias(\"avg_price\"),\n",
    "            count(\"*\").alias(\"total_days\")\n",
    "        )\n",
    "        wind_summary.show(truncate=False)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Query failed: {e}\")\n",
    "    print(\"This is expected if Delta tables don't exist - using sample data for POC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3299da01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stop Spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
