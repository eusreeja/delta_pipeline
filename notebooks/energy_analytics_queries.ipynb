{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56a1e234",
   "metadata": {},
   "source": [
    "# Energy Data Analytics - Delta Lake Queries\n",
    "\n",
    "This notebook demonstrates reading Delta format data and executing three core energy analytics queries using Spark SQL:\n",
    "\n",
    "1. **Daily Production Trends** - Daily electricity production by production type\n",
    "2. **Underperformance Prediction Features** - ML features for energy production forecasting\n",
    "3. **Wind Price Analysis** - Wind power production vs electricity prices\n",
    "\n",
    "## Data Sources\n",
    "- Delta tables in the Gold layer: `gold_fact_power`, `gold_dim_production_type`, `gold_fact_power_30min_agg`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eeb6fe5",
   "metadata": {},
   "source": [
    "## 1. Setup Spark Session and Delta Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e77ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Create Spark session with Delta Lake configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"EnergyAnalyticsDelta\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark Session created with Delta Lake support\")\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Application Name: {spark.sparkContext.appName}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f82e87d",
   "metadata": {},
   "source": [
    "## 2. Read Delta Lake Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae156ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Delta table paths (update these paths to match your environment)\n",
    "delta_base_path = \"/Users/srsu/Downloads/spark_datapipeline/delta_lake\"\n",
    "\n",
    "# Read Delta tables from the Gold layer\n",
    "print(\"üìä Reading Delta tables...\")\n",
    "\n",
    "try:\n",
    "    # Read dimension table\n",
    "    gold_dim_production_type = spark.read.format(\"delta\").load(f\"{delta_base_path}/gold/gold_dim_production_type\")\n",
    "    gold_dim_production_type.createOrReplaceTempView(\"gold_dim_production_type\")\n",
    "    print(f\"Loaded gold_dim_production_type: {gold_dim_production_type.count()} records\")\n",
    "    \n",
    "    # Read daily fact table\n",
    "    gold_fact_power = spark.read.format(\"delta\").load(f\"{delta_base_path}/gold/gold_fact_power\")\n",
    "    gold_fact_power.createOrReplaceTempView(\"gold_fact_power\")\n",
    "    print(f\"Loaded gold_fact_power: {gold_fact_power.count()} records\")\n",
    "    \n",
    "    # Read 30-minute aggregated fact table\n",
    "    gold_fact_power_30min_agg = spark.read.format(\"delta\").load(f\"{delta_base_path}/gold/gold_fact_power_30min_agg\")\n",
    "    gold_fact_power_30min_agg.createOrReplaceTempView(\"gold_fact_power_30min_agg\")\n",
    "    print(f\"Loaded gold_fact_power_30min_agg: {gold_fact_power_30min_agg.count()} records\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Could not read Delta tables: {e}\")\n",
    "    print(\"Creating sample data for POC demonstration...\")\n",
    "    \n",
    "    # Create sample data for demonstration\n",
    "    # Dimension data\n",
    "    from pyspark.sql.types import *\n",
    "    \n",
    "    dim_data = [\n",
    "        (1, \"Solar\", \"Solar\", \"Renewable\", \"Uncontrollable\", True),\n",
    "        (2, \"Wind_Onshore\", \"Wind_Onshore\", \"Renewable\", \"Uncontrollable\", True),\n",
    "        (3, \"Wind_Offshore\", \"Wind_Offshore\", \"Renewable\", \"Uncontrollable\", True),\n",
    "        (4, \"Nuclear\", \"Nuclear\", \"Nuclear\", \"Controllable\", True)\n",
    "    ]\n",
    "    \n",
    "    dim_schema = StructType([\n",
    "        StructField(\"production_type_id\", IntegerType(), True),\n",
    "        StructField(\"production_type\", StringType(), True),\n",
    "        StructField(\"production_plant_name\", StringType(), True),\n",
    "        StructField(\"energy_category\", StringType(), True),\n",
    "        StructField(\"controllability_type\", StringType(), True),\n",
    "        StructField(\"active_flag\", BooleanType(), True)\n",
    "    ])\n",
    "    \n",
    "    spark.createDataFrame(dim_data, dim_schema).createOrReplaceTempView(\"gold_dim_production_type\")\n",
    "    print(\"Created sample gold_dim_production_type\")\n",
    "\n",
    "print(\"\\nüéØ Delta tables loaded and ready for queries!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe46d90d",
   "metadata": {},
   "source": [
    "## 3. Query 1: Daily Production Trends\n",
    "\n",
    "This query analyzes daily electricity production trends by production type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b0b7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 1: Daily Production Trends\n",
    "daily_production_query = \"\"\"\n",
    "SELECT\n",
    "  f.year,\n",
    "  f.month,\n",
    "  f.day,\n",
    "  d.production_plant_name AS production_type,\n",
    "  SUM(f.electricity_produced) AS total_daily_production\n",
    "FROM gold_fact_power f\n",
    "JOIN gold_dim_production_type d ON f.production_type_id = d.production_type_id\n",
    "WHERE f.country = 'de'\n",
    "GROUP BY f.year, f.month, f.day, d.production_plant_name\n",
    "ORDER BY f.year, f.month, f.day, d.production_plant_name\n",
    "\"\"\"\n",
    "\n",
    "print(\"üîç Executing Query 1: Daily Production Trends\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    daily_trends_df = spark.sql(daily_production_query)\n",
    "    \n",
    "    print(f\"Query executed successfully!\")\n",
    "    print(f\"Results: {daily_trends_df.count()} records found\")\n",
    "    \n",
    "    print(\"\\nSample Results:\")\n",
    "    daily_trends_df.show(10, truncate=False)\n",
    "    \n",
    "    # Show schema\n",
    "    print(\"\\nSchema:\")\n",
    "    daily_trends_df.printSchema()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Query failed: {e}\")\n",
    "    print(\"This is expected if Delta tables don't exist - using sample data for POC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23621cce",
   "metadata": {},
   "source": [
    "## 4. Query 2: Underperformance Prediction Features\n",
    "\n",
    "This query generates ML features for predicting energy production underperformance with lag features and rolling averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a94f0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 2: Underperformance Prediction Features\n",
    "underperformance_query = \"\"\"\n",
    "SELECT\n",
    "    f.timestamp_30min,\n",
    "    f.production_type_id,\n",
    "    d.production_plant_name,\n",
    "    d.energy_category,\n",
    "    d.controllability_type,\n",
    "    f.total_electricity_produced,\n",
    "    f.year, f.month, f.day, f.hour, f.minute_interval_30,\n",
    "    LAG(f.total_electricity_produced, 48) OVER (PARTITION BY f.production_type_id ORDER BY f.timestamp_30min) AS lag_1d,\n",
    "    LAG(f.total_electricity_produced, 336) OVER (PARTITION BY f.production_type_id ORDER BY f.timestamp_30min) AS lag_1w,\n",
    "    AVG(f.total_electricity_produced) OVER (\n",
    "        PARTITION BY f.production_type_id, f.hour, f.minute_interval_30\n",
    "        ORDER BY f.timestamp_30min\n",
    "        RANGE BETWEEN 336 PRECEDING AND 1 PRECEDING\n",
    "    ) AS rolling_7d_avg\n",
    "FROM gold_fact_power_30min_agg f\n",
    "JOIN gold_dim_production_type d ON f.production_type_id = d.production_type_id\n",
    "WHERE f.country = 'de' AND d.active_flag = TRUE\n",
    "\"\"\"\n",
    "\n",
    "print(\"üîç Executing Query 2: ML Features for Underperformance Prediction\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "try:\n",
    "    underperformance_query_df = spark.sql(underperformance_query)\n",
    "    \n",
    "    print(f\"Query executed successfully!\")\n",
    "    print(f\"Results: {underperformance_query_df.count()} records found\")\n",
    "    \n",
    "    print(\"\\nSample ML Features:\")\n",
    "    underperformance_query_df.show(5, truncate=False)\n",
    "    \n",
    "    # Show feature statistics\n",
    "    print(\"\\nFeature Statistics:\")\n",
    "    underperformance_query_df.select(\"total_electricity_produced\", \"lag_1d\", \"lag_1w\", \"rolling_7d_avg\").describe().show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\" Query failed: {e}\")\n",
    "    print(\"This is expected if 30-minute Delta table doesn't exist - using sample data for POC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd91dd0",
   "metadata": {},
   "source": [
    "## 5. Query 3: Wind Price Analysis\n",
    "\n",
    "This query analyzes the relationship between wind power production (offshore and onshore) and electricity prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68508a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 3: Wind Price Analysis\n",
    "wind_price_query = \"\"\"\n",
    "SELECT\n",
    "  f.year, f.month, f.day,\n",
    "  d.production_plant_name AS production_type,\n",
    "  SUM(f.electricity_produced) AS total_daily_production_mw,\n",
    "  AVG(f.electricity_price) AS avg_daily_price_eur_per_mwh\n",
    "FROM gold_fact_power f\n",
    "JOIN gold_dim_production_type d ON f.production_type_id = d.production_type_id\n",
    "WHERE f.country = 'de'\n",
    "  AND d.production_plant_name IN ('Wind_Offshore', 'Wind_Onshore') \n",
    "  AND d.active_flag = TRUE \n",
    "GROUP BY f.year, f.month, f.day, d.production_plant_name\n",
    "ORDER BY f.year, f.month, f.day, d.production_plant_name\n",
    "\"\"\"\n",
    "\n",
    "print(\"üîç Executing Query 3: Wind Power vs Price Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    wind_analysis_df = spark.sql(wind_price_query)\n",
    "    \n",
    "    print(f\"Query executed successfully!\")\n",
    "    print(f\"Results: {wind_analysis_df.count()} records found\")\n",
    "    \n",
    "    print(\"\\nWind Power vs Price Results:\")\n",
    "    wind_analysis_df.show(10, truncate=False)\n",
    "    \n",
    "    # Summary statistics by wind type\n",
    "    if wind_analysis_df.count() > 0:\n",
    "        print(\"\\nSummary by Wind Type:\")\n",
    "        wind_summary = wind_analysis_df.groupBy(\"production_type\").agg(\n",
    "            avg(\"total_daily_production_mw\").alias(\"avg_production\"),\n",
    "            avg(\"avg_daily_price_eur_per_mwh\").alias(\"avg_price\"),\n",
    "            count(\"*\").alias(\"total_days\")\n",
    "        )\n",
    "        wind_summary.show(truncate=False)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Query failed: {e}\")\n",
    "    print(\"This is expected if Delta tables don't exist - using sample data for POC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3299da01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stop Spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
